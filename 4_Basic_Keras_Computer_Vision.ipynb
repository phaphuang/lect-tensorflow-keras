{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4.Basic Keras - Computer Vision.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgRPMOh0gtxA"
      },
      "source": [
        "## **Build Fashion image recognition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRJb7wOAfzpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eb6c8aa-60e1-4309-fe41-189778cd9cc0"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X99pbTAd3d0i"
      },
      "source": [
        "source: https://medium.com/nanonets/how-to-classify-fashion-images-easily-using-convnets-81e1e0019ffe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS2kC5VGxg5I"
      },
      "source": [
        "### Fashion dataset\n",
        "https://github.com/zalandoresearch/fashion-mnist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsV72GvugjYO"
      },
      "source": [
        "fashionmnist = tf.keras.datasets.fashion_mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqfsyILYGuHP"
      },
      "source": [
        "#### tf.keras.datasets\n",
        "https://keras.io/api/datasets/ <br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMy1_lmPgntt"
      },
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = fashionmnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4xvIL1YxQSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a86f5277-7fee-4923-a449-705179bd57fb"
      },
      "source": [
        "training_images.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGojb5vzxTg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d7a152-e65f-4eac-964d-cd9ab5962428"
      },
      "source": [
        "test_images.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guFTa2bWhJ1H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9926c8dc-7976-4519-c7fe-223adb4094ac"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(training_images[0], cmap=\"gray\")\n",
        "print(training_labels[0])\n",
        "print(training_images[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0\n",
            "    0   1   4   0   0   0   0   1   1   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62\n",
            "   54   0   0   0   1   3   4   0   0   3]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134\n",
            "  144 123  23   0   0   0   0  12  10   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178\n",
            "  107 156 161 109  64  23  77 130  72  15]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216\n",
            "  216 163 127 121 122 146 141  88 172  66]\n",
            " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229\n",
            "  223 223 215 213 164 127 123 196 229   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228\n",
            "  235 227 224 222 224 221 223 245 173   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198\n",
            "  180 212 210 211 213 223 220 243 202   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192\n",
            "  169 227 208 218 224 212 226 197 209  52]\n",
            " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203\n",
            "  198 221 215 213 222 220 245 119 167  56]\n",
            " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240\n",
            "  232 213 218 223 234 217 217 209  92   0]\n",
            " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219\n",
            "  222 221 216 223 229 215 218 255  77   0]\n",
            " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208\n",
            "  211 218 224 223 219 215 224 244 159   0]\n",
            " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230\n",
            "  224 234 176 188 250 248 233 238 215   0]\n",
            " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223\n",
            "  255 255 221 234 221 211 220 232 246   0]\n",
            " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221\n",
            "  188 154 191 210 204 209 222 228 225   0]\n",
            " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117\n",
            "  168 219 221 215 217 223 223 224 229  29]\n",
            " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245\n",
            "  239 223 218 212 209 222 220 221 230  67]\n",
            " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216\n",
            "  199 206 186 181 177 172 181 205 206 115]\n",
            " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191\n",
            "  195 191 198 192 176 156 167 177 210  92]\n",
            " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209\n",
            "  210 210 211 188 188 194 192 216 170   0]\n",
            " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179\n",
            "  182 182 181 176 166 168  99  58   0   0]\n",
            " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8BtaexbhZn-"
      },
      "source": [
        "### Normalizing the values in to range between 0 to 1\n",
        "You will see that the values in the image are between 0 to 255. But a neural network works better with normalized data (between 0 and 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_H1s9n_hTDS"
      },
      "source": [
        "training_images = training_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhTLCeo5-2Qi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae1516e4-0024-4673-ab61-65d2fff58db7"
      },
      "source": [
        "training_images.shape, training_labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZJFcymwzY3W"
      },
      "source": [
        "### Dense Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n58k91TtzreR"
      },
      "source": [
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kJr3SczGn6f"
      },
      "source": [
        "![](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/73_blog_image_1.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBn3ko2aiJKN"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SZ6mcisG0h-"
      },
      "source": [
        "#### **[Multi-label vs. Multi-class Classification](https://glassboxmedicine.com/2019/05/26/classification-sigmoid-vs-softmax/)**\n",
        "We convert a classifier’s raw output values into probabilities using either a sigmoid function or a softmax function.\n",
        "#### **Multi-Label Classification** Problem = More than one right answer\n",
        "When we’re building  a classifier for a problem with more than one right answer, we apply a **sigmoid** function to each element of the raw output independently.\n",
        "$$\\sigma(z_j)=\\frac{e^{z_j}}{1+e^{z_j}}$$\n",
        "![](https://glassboxmedicine.files.wordpress.com/2019/05/sigmoidvssoftmax2-1.png?w=768)\n",
        "#### **Multi-Class Classification** Problem = Only one right answer\n",
        "- When we’re building a classifier for problems with only one right answer, we apply a **softmax** to the raw outputs.\n",
        "- the different probabilities produced by the softmax function are interrelated.\n",
        "$$\\text{softmax}(z_j)=\\frac{e^{z_j}}{\\sum^K_{k=1}e^{z_k}}; \\text{for }j=1,...,K$$\n",
        "![](https://glassboxmedicine.files.wordpress.com/2019/05/sigmoidvssoftmax3.png?w=768)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYYF_ba0H4dR"
      },
      "source": [
        "#### Testing softmax function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUbHR6HsHBrz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "901a7179-5e78-4509-9bb1-1ea4689f7b95"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z)\n",
        "    sum_exp_z = np.sum(exp_z, axis=0)\n",
        "    return exp_z / sum_exp_z\n",
        "\n",
        "z = [1., 2., 3.]\n",
        "print(\"Before apply softmax function:\", z)\n",
        "\n",
        "softmax_z = softmax(z)\n",
        "print(\"After apply softmax function:\", softmax_z)\n",
        "\n",
        "print(\"Sum of softmax:\", softmax_z.sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before apply softmax function: [1.0, 2.0, 3.0]\n",
            "After apply softmax function: [0.09003057 0.24472847 0.66524096]\n",
            "Sum of softmax: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHG_r4WZibZX"
      },
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHs7Z9yqIqaJ"
      },
      "source": [
        "Available losses in Keras API\n",
        "https://keras.io/api/losses/ <br>\n",
        "**Sparse Categorical Crossentropy** - Computes the crossentropy loss between the labels and predictions.\n",
        "$$\\text{Loss} = -\\sum^C_{c=1}y_i\\log(\\hat{y_i})$$\n",
        "$C$ is the total classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMhbQPZKwsIy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "797c4f3e-4882-4580-d3b2-a2bbdf414370"
      },
      "source": [
        "model.fit(training_images, training_labels, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4984 - accuracy: 0.8256\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3742 - accuracy: 0.8649\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3344 - accuracy: 0.8791\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3139 - accuracy: 0.8843\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2915 - accuracy: 0.8924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3ea04527f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-QCL-OOw7dR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b058d306-cb4b-4426-e2fe-32ff6e280648"
      },
      "source": [
        "model.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 2ms/step - loss: 0.3630 - accuracy: 0.8711\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3630017340183258, 0.8711000084877014]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxPaPBVyzQ7u"
      },
      "source": [
        "### Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em6ub76KKM6H"
      },
      "source": [
        "#### **Convolution**\n",
        "![](https://miro.medium.com/max/1920/1*D6iRfzDkz-sEzyjYoVZ73w.gif)\n",
        "![](https://sgugger.github.io/images/art4_one_conv.png)\n",
        "[Try with different kernels](https://setosa.io/ev/image-kernels/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD4zpTs4P5ua"
      },
      "source": [
        "Convolution layer ([Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)) <br>\n",
        "More information about Convolutional Neural Network [link](https://www.youtube.com/playlist?list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kc5nzCrP0cE"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibOQ1e7k12T4"
      },
      "source": [
        "from tensorflow.keras.layers import Flatten, Dense, Conv2D\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMCgqu7S-Caa"
      },
      "source": [
        "training_images = training_images.reshape(training_images.shape[0], training_images.shape[1], training_images.shape[2], 1)\n",
        "test_images = test_images.reshape(test_images.shape[0], test_images.shape[1], test_images.shape[2], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRJ2QJEcE5CS"
      },
      "source": [
        "#### 1 Conv CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4NgvZvfxHUn"
      },
      "source": [
        "conv_model = Sequential()\n",
        "conv_model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=training_images.shape[1:]))\n",
        "conv_model.add(Flatten())\n",
        "conv_model.add(Dense(128, activation='relu'))\n",
        "conv_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "conv_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1v31-153HnN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b1dc1ab-3d36-430e-cb02-e86120991b5a"
      },
      "source": [
        "conv_model.fit(training_images, training_labels, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3819 - accuracy: 0.8637\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2537 - accuracy: 0.9064\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1999 - accuracy: 0.9252\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1599 - accuracy: 0.9406\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1257 - accuracy: 0.9538\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3ea02814e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS_SJ398FQpO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4475904a-71b6-4e1f-cc6c-e818eebc2d78"
      },
      "source": [
        "score2 = conv_model.evaluate(test_images, test_labels)\n",
        "print('Test loss:', score2[0])\n",
        "print('Test accuracy:', score2[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3033 - accuracy: 0.9083\n",
            "Test loss: 0.30334171652793884\n",
            "Test accuracy: 0.90829998254776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEWIaCRjQoIV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "098e6a7b-a0da-4db3-9941-61f48cc9b607"
      },
      "source": [
        "conv_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 21632)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               2769024   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 2,770,634\n",
            "Trainable params: 2,770,634\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz4ivu7C_SFd"
      },
      "source": [
        "#### One-Hot encoding in label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUUIcCox_RQt"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# one hot encode target values\n",
        "trainY = to_categorical(training_labels)\n",
        "testY = to_categorical(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY-CoSkLTYfK"
      },
      "source": [
        "#### One-Hot encoding\n",
        "![](https://static.packt-cdn.com/products/9781838556334/graphics/C12624_06_04.jpg)\n",
        "https://www.youtube.com/watch?v=BecEHOVmx9o"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjHp3Z7PThx0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c63a42-bc52-4c19-b3c7-1177c2e043a5"
      },
      "source": [
        "print(\"Before encoding:\\n\", training_labels[:3])\n",
        "print(\"After encoding:\\n\", trainY[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before encoding:\n",
            " [9 0 0]\n",
            "After encoding:\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkudyv7JFn1r"
      },
      "source": [
        "from tensorflow.keras.layers import MaxPooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XncBhxd2Jw1E"
      },
      "source": [
        "Max pooling layer ([MaxPooling2d](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)) <br>\n",
        "![](https://austingwalters.com/wp-content/uploads/2019/01/max-pooling.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sngEK1b_JR6h"
      },
      "source": [
        "    # Add convolution 2D\n",
        "cnn_one_hot = Sequential()\n",
        "cnn_one_hot.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "cnn_one_hot.add(MaxPooling2D((2, 2)))\n",
        "cnn_one_hot.add(Flatten())\n",
        "cnn_one_hot.add(Dense(100, activation='relu'))\n",
        "cnn_one_hot.add(Dense(10, activation='softmax'))\n",
        "\n",
        "cnn_one_hot.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebxy2PNiShiU"
      },
      "source": [
        "If your targets are one-hot encoded, use \"categorical_crossentropy\". But if your targets are integers, use \"sparse_categorical_crossentropy\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ig6MB_N_mLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "409241af-6249-4565-bfeb-7f5a5b1eaf79"
      },
      "source": [
        "cnn_one_hot.fit(training_images, trainY, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4064 - accuracy: 0.8575\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2740 - accuracy: 0.9003\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2291 - accuracy: 0.9164\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1968 - accuracy: 0.9272\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1699 - accuracy: 0.9374\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3ea01bd550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFChaIIC_pC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b105255-d3d6-4df3-f760-815c10070ee3"
      },
      "source": [
        "score3 = cnn_one_hot.evaluate(test_images, testY, verbose=0)\n",
        "print('Test loss:', score3[0])\n",
        "print('Test accuracy:', score3[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.24243171513080597\n",
            "Test accuracy: 0.9151999950408936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gosxppDcQstr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d2ed1de-ea56-4663-909c-da6190d972e3"
      },
      "source": [
        "cnn_one_hot.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 5408)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               540900    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 542,230\n",
            "Trainable params: 542,230\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oz4QeIzFBhl"
      },
      "source": [
        "#### 3 Conv CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJRjcJoYzjEQ"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=10jDCwmNndYfM2s7gVQt4MeTpKt6zr-mt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxLVYUOM3NjA"
      },
      "source": [
        "# Add convolution 2D\n",
        "cnn3conv = Sequential()\n",
        "cnn3conv.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=training_images.shape[1:]))\n",
        "cnn3conv.add(MaxPooling2D((2, 2)))\n",
        "cnn3conv.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "cnn3conv.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn3conv.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "cnn3conv.add(Flatten())\n",
        "cnn3conv.add(Dense(128, activation='relu'))\n",
        "cnn3conv.add(Dense(10, activation='softmax'))\n",
        "\n",
        "cnn3conv.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2-IwnUNFe9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfc7fb3a-50f0-4e78-8cc1-b838b7211a91"
      },
      "source": [
        "cnn3conv.fit(training_images, training_labels, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4690 - accuracy: 0.8282\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3004 - accuracy: 0.8893\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2533 - accuracy: 0.9051\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2227 - accuracy: 0.9183\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1978 - accuracy: 0.9267\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3ea0043208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU3Km5oWI8Up",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394fba79-54c1-4d84-d0bc-64ae6f413e26"
      },
      "source": [
        "score4 = cnn3conv.evaluate(test_images, test_labels, verbose=0)\n",
        "print('Test loss:', score4[0])\n",
        "print('Test accuracy:', score4[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.2658214867115021\n",
            "Test accuracy: 0.9063000082969666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0QwXG1RGtrM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25117ae2-f5e2-4faf-8cc1-0eebcb505964"
      },
      "source": [
        "cnn3conv.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 3, 3, 128)         73856     \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 1152)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               147584    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 241,546\n",
            "Trainable params: 241,546\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5Q4Gp-J0MCh"
      },
      "source": [
        "## **Transfer Learning**\n",
        "Instead of training from zero, why not train from existing model (knowledge)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb2yxRxi0NWr"
      },
      "source": [
        "# example of using a pre-trained model as a classifier\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "from keras.applications.vgg16 import VGG16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPFF-HId9wuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "025531d1-fe44-4a87-a0ba-14b6dc9fca8a"
      },
      "source": [
        "from io import BytesIO\n",
        "from urllib import request\n",
        "from PIL import Image\n",
        "\n",
        "# load an image from file\n",
        "image_url =  'https://bloximages.newyork1.vip.townnews.com/gazette.com/content/tncms/assets/v3/editorial/e/7f/e7ff6af0-929e-11e9-bb60-3fb6efa3366b/5d0a479b8c78c.image.jpg?crop=1247%2C1247%2C207%2C0&resize=1247%2C1247&order=crop%2Cresize' #@param {type:\"string\"}\n",
        "res = request.urlopen(image_url).read()\n",
        "image = Image.open(BytesIO(res)).resize((224, 224))\n",
        "# convert the image pixels to a numpy array\n",
        "image = img_to_array(image)\n",
        "# reshape data for the model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "image.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t_b5-xEAsN4"
      },
      "source": [
        "# prepare the image for the VGG model\n",
        "image = preprocess_input(image)\n",
        "# load the model\n",
        "model = VGG16()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHpux2XPAz-o"
      },
      "source": [
        "#### **VGG16 Network architecture**\n",
        "VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper “[Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)”. The model achieves 92.7% top-5 test accuracy in [ImageNet](http://www.image-net.org/), which is a dataset of over 14 million images belonging to 1000 classes.\n",
        "![](https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png)\n",
        "#### 1,000 ImageNet dataset classes https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
        "#### More models: https://keras.io/api/applications/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MszFSlPCA9tL"
      },
      "source": [
        "# predict the probability across all output classes\n",
        "yhat = model.predict(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqFfraItCc7B"
      },
      "source": [
        "# convert the probabilities to class labels\n",
        "label = decode_predictions(yhat) # https://www.tensorflow.org/api_docs/python/tf/keras/applications/imagenet_utils/decode_predictions\n",
        "# retrieve the most likely result, e.g. highest probability\n",
        "label = label[0][0]\n",
        "# print the classification\n",
        "print('%s (%.2f%%)' % (label[1], label[2]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYpN7LmH-UQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddc87ea0-5478-4aeb-93a6-05f34d6a1075"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt_EuycE_zTY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5.NLP_Word Encoding-Text to Sequence-Padding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xo5y_q52S5_"
      },
      "source": [
        "## Word Encoding\n",
        "Everyone has a name --> **Word too** !! <br>\n",
        "Considering these two sentences.\n",
        "<br>\n",
        "''' **I love my dog** '''\n",
        "<br>\n",
        "''' **I love my cat** '''\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxWG3wTGrJft",
        "outputId": "306906ad-adbf-43ed-b4f5-f714f6da04dc"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'I love my dog',\n",
        "    'I love my cat'\n",
        "]\n",
        "\n",
        "# num_words is the maximum number of words to be encoded\n",
        "# Tokenizer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "# Remember the word which was capitalized will convert to the lower case --> Tokenizer did it for you\n",
        "# Also stripts punctuation out\n",
        "print(\"Word Index:\", word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word Index: {'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRmHPkThh0CD"
      },
      "source": [
        "### Tuple, List, Dictionary\n",
        "List is the standard form for collecting the data in Python. The data can be updated, added, or removed in this list ex. L = [1, 2, 3, 4, 5] <br>\n",
        "Tuple faster than list in query but the value in tuple can not be modified ex. T = (1, 2, 3, 4, 5) <br>\n",
        "Dictionary has key and value ex. D = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igQZaC4iM2SF"
      },
      "source": [
        "Adding **You love my dog** with exclamation and question mark after **dog**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOy-sEdl2R03",
        "outputId": "11a3ae3d-cb4b-4113-9de3-8842f0bfe6e5"
      },
      "source": [
        "sentences = [\n",
        "    'I love my dog',\n",
        "    'I love my cat',\n",
        "    'You love my dog!?'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "# With new corpus of text is including 'you' and 'dog' --> exclamation and question mark did not impact 'dog' word \n",
        "print(\"Word Index:\", word_index) # Return the dictionary of the words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word Index: {'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL3Zu4X0PJSc"
      },
      "source": [
        "## Text to Sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR9-BBiqPLez",
        "outputId": "455006f8-b3b6-4c2f-f9d1-6fcc2dc0239c"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'I love my dog',\n",
        "    'I love my cat',\n",
        "    'You love my dog!?',\n",
        "    'Do you think my dog is crazy^^?'\n",
        "\n",
        "]\n",
        "\n",
        "# num_words is the maximum number of words to be encoded\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# texts_to_sequences will convert all words in to a set of sequences and return as the list\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "print(\"Word Index:\", word_index)\n",
        "print(\"\\nSequences:\\n\", sequences) # --> list of sentences which is encoded into integer lists"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word Index: {'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'crazy': 10}\n",
            "\n",
            "Sequences:\n",
            " [[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0gA61kmWImG"
      },
      "source": [
        "**texts_to_sequences** can take any set of sentences and encode them.\n",
        "<br>\n",
        "Just think about when we train the neural network by Keras with a corpus of texts and text has a word index generated from it. For testing, we have to encode the text with the same word index else it would be meaningless."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9bOnOH1UoOJ",
        "outputId": "39b7cd75-af0d-47bc-edcd-87502386db95"
      },
      "source": [
        "test_data = [\n",
        "    'i really love my dog',\n",
        "    'my dog loves my aunt'\n",
        "]\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(\"\\nSequences:\\n\", test_seq) # the output result can encode only the word in our corpus --> it ignores the unseen words --> end up with 'my dog my' instead of 'my dog loves my aunt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Sequences:\n",
            " [[4, 2, 1, 3], [1, 3, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6HWaDXtXw3s"
      },
      "source": [
        "We learn that <br>\n",
        "1. We really need a lot of data to get a broad vocabulary. If not we gonna end up likes **my dog my** above. <br>\n",
        "2. Instead of ignoring the unseen words, we can put a special value in when we encounter the unseen words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPlZ6ggoXSO2",
        "outputId": "21f8f50b-a23b-4b56-aa76-123cace649f8"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentenses = [\n",
        "    'I love my dog',\n",
        "    'I love my cat',\n",
        "    'You love my dog!?',\n",
        "    'Do you think my dog is crazy^^?'\n",
        "]\n",
        "\n",
        "# Put a word '<OOV>' for unseen words\n",
        "# --> can use any word you like but please be noted that it is unique and distinct that is not confused with a real word.\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "test_data = [\n",
        "    'i really love my dog',\n",
        "    'my dog loves my aunt'\n",
        "]\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(\"Word Index:\", word_index)\n",
        "print(\"\\nSequence:\\n\", test_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word Index: {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'crazy': 11}\n",
            "\n",
            "Sequence:\n",
            " [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_TKp7tkd14T"
      },
      "source": [
        "Still not great but it is doing better!! <br>\n",
        "The more words in corpus grows --> The more words are in the index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcgC5SYMeOiE"
      },
      "source": [
        "## Padding\n",
        "When we build a neural network --> we need the the input data to be uniform in size before feed them into the network for training <br> \n",
        "For working with images in batch, all images should have the same size!! <br>\n",
        "... Same to text data --> Padding will help you do this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmdS-YQMdEF5",
        "outputId": "40ca7e28-e65d-47b3-e5c2-eca8c20a615f"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentenses = [\n",
        "    'I love my dog',\n",
        "    'I love my cat',\n",
        "    'You love my dog!?',\n",
        "    'Do you think my dog is crazy^^?'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Once the tokenizer has created the sequences, these sequences can be passed to pad sequences in order to have them padded\n",
        "padded = pad_sequences(sequences)\n",
        "print(\"Word Index:\", word_index)\n",
        "print(\"\\nSequences:\\n\", sequences)\n",
        "print(\"\\nPadded:\\n\", padded) # The list of sentences can be passed it out as the matrix --> each row has the same length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word Index: {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'crazy': 11}\n",
            "\n",
            "Sequences:\n",
            " [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
            "\n",
            "Padded:\n",
            " [[ 0  0  0  5  3  2  4]\n",
            " [ 0  0  0  5  3  2  7]\n",
            " [ 0  0  0  6  3  2  4]\n",
            " [ 8  6  9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRDwdZkBjK_d",
        "outputId": "368ec1a6-eac4-499f-8d68-6250a17a48b6"
      },
      "source": [
        "# Sometimes the padding will be after the sentences by adding parameter 'padding'\n",
        "padded = pad_sequences(sequences, padding='post')\n",
        "print(\"Word Index:\", word_index)\n",
        "print(\"\\nSequences:\\n\", sequences)\n",
        "print(\"\\nPadded:\\n\", padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word Index: {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'crazy': 11}\n",
            "\n",
            "Sequences:\n",
            " [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
            "\n",
            "Padded:\n",
            " [[ 5  3  2  4  0  0  0]\n",
            " [ 5  3  2  7  0  0  0]\n",
            " [ 6  3  2  4  0  0  0]\n",
            " [ 8  6  9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ9gPA-0j7yj",
        "outputId": "b9939ce6-a8e3-4c56-d6ee-6c17a38a969a"
      },
      "source": [
        "# With the maxlen, you can specify the number of word in your sentences -> But the information will lost in the begining if you set 'padding' to 'post'\n",
        "padded = pad_sequences(sequences, padding='post', maxlen=5)\n",
        "print(\"Word Index:\", word_index)\n",
        "print(\"Sequences:\\n\", sequences)\n",
        "print(\"Padded:\\n\", padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word Index: {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'crazy': 11}\n",
            "Sequences:\n",
            " [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
            "Padded:\n",
            " [[ 5  3  2  4  0]\n",
            " [ 5  3  2  7  0]\n",
            " [ 6  3  2  4  0]\n",
            " [ 9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pv0cLDlvkMgM",
        "outputId": "dc986647-41fa-4f67-95bb-9384de285083"
      },
      "source": [
        "# With the maxlen, you can specify the number of word in your sentences -> With truncating the information will lost from behind instead\n",
        "padded = pad_sequences(sequences, padding='post', maxlen=5, truncating='post')\n",
        "print(\"Word Index:\", word_index)\n",
        "print(\"\\nSequences:\\n\", sequences)\n",
        "print(\"\\nPadded:\\n\", padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word Index: {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'crazy': 11}\n",
            "\n",
            "Sequences:\n",
            " [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
            "\n",
            "Padded:\n",
            " [[5 3 2 4 0]\n",
            " [5 3 2 7 0]\n",
            " [6 3 2 4 0]\n",
            " [8 6 9 2 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwKpML9ulCx1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}